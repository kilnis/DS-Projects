{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO34K2AevKiuzS8pvHWQNFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilnis/DS-Projects/blob/main/%D0%A2%D0%97_%D0%9A%D1%81%D0%B5%D0%BD%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RREJxHkik2W",
        "outputId": "1cc3c69b-a4e7-472a-8db2-ac61fced3f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=f03774188566b8c7eb7c344e1f9116d26981afc3522dac7cabbbe0c63ce638ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1|Alice|\n",
            "|  2|  Bob|\n",
            "|  3|Cathy|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "# Импорт необходимых библиотек\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "# Инициализация Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Пример создания DataFrame\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
        "\n",
        "# Показать DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql import Window\n"
      ],
      "metadata": {
        "id": "DoTE1Onui9Eo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"test_sql\") \\\n",
        "    .getOrCreate()\n",
        "# spark = init_spark(\"test_sql\", spark_version=\"2.4.8\")\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, lit\n",
        "from pyspark.sql import Window"
      ],
      "metadata": {
        "id": "5VLVsre15acs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обязательные требования к решению:\n",
        "1) Решение задач необходимо сделать в общем виде, для произвольного набора данных.\n",
        "2) Решение необходимо выполнить в данном JN с выводом результата на экран.  "
      ],
      "metadata": {
        "id": "EgGzSrXo5SWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача №1"
      ],
      "metadata": {
        "id": "cstSLS4Bn2Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Набор исходных данных для задачи №1\n",
        "df = spark.sql(\"\"\"\n",
        "select 'Красный' as color union all\n",
        "select 'Зеленый' as color union all\n",
        "select 'Синий' as color union all\n",
        "select 'Желтый' as color union all\n",
        "select 'Фиолетовый' as color union all\n",
        "select 'Пурпурный' as color union all\n",
        "select 'Белый' as color\n",
        "\"\"\")\n",
        "df.createOrReplaceTempView(\"table1\")\n",
        "df.show(20,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jPNjDtPnvv0",
        "outputId": "f0cac65c-c8f7-4aa3-c8d9-b9d261e826e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|color     |\n",
            "+----------+\n",
            "|Красный   |\n",
            "|Зеленый   |\n",
            "|Синий     |\n",
            "|Желтый    |\n",
            "|Фиолетовый|\n",
            "|Пурпурный |\n",
            "|Белый     |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comby = spark.sql(\"\"\"\n",
        "SELECT a.color AS color_1, b.color AS color_2\n",
        "FROM table1 a CROSS JOIN table1 b\n",
        "WHERE a.color < b.color\n",
        "\"\"\")\n",
        "\n",
        "comby.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZKJCxwVnty4",
        "outputId": "2d426645-6381-4f12-a04c-ba7e5c615022"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+\n",
            "|color_1  |color_2   |\n",
            "+---------+----------+\n",
            "|Красный  |Синий     |\n",
            "|Красный  |Фиолетовый|\n",
            "|Красный  |Пурпурный |\n",
            "|Зеленый  |Красный   |\n",
            "|Зеленый  |Синий     |\n",
            "|Зеленый  |Фиолетовый|\n",
            "|Зеленый  |Пурпурный |\n",
            "|Синий    |Фиолетовый|\n",
            "|Желтый   |Красный   |\n",
            "|Желтый   |Зеленый   |\n",
            "|Желтый   |Синий     |\n",
            "|Желтый   |Фиолетовый|\n",
            "|Желтый   |Пурпурный |\n",
            "|Пурпурный|Синий     |\n",
            "|Пурпурный|Фиолетовый|\n",
            "|Белый    |Красный   |\n",
            "|Белый    |Зеленый   |\n",
            "|Белый    |Синий     |\n",
            "|Белый    |Желтый    |\n",
            "|Белый    |Фиолетовый|\n",
            "+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача №2"
      ],
      "metadata": {
        "id": "83W-LYE-ni59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть таблица в которой каждая запись действует определенный период с date_from по date_to.\n",
        "Необходимо выбрать записи которые попадают в промежуток времени с 2019-03-01 по 2021-03-01\n",
        "т.е. действовали хотя-бы один день в этом промежутке.\n",
        "Задачу необходимо решить применяя не более 3 условий фильтрации данных"
      ],
      "metadata": {
        "id": "LOVAJwMem_QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ([(\"1\",\"2000-01-01\",\"2032-01-02\",\"100\"),\n",
        "         (\"2\",\"2010-01-01\",\"2020-01-02\",\"200\"),\n",
        "         (\"3\",\"2011-01-01\",\"2020-01-02\",\"300\"),\n",
        "         (\"4\",\"2018-01-01\",\"2019-01-02\",\"400\"),\n",
        "         (\"5\",\"2019-05-01\",\"2020-05-02\",\"500\"),\n",
        "         (\"6\",\"2020-01-01\",\"2030-02-02\",\"600\"),\n",
        "         (\"7\",\"2021-02-01\",\"2030-01-02\",\"700\")\n",
        "       ])\n",
        "columns = [\"id\",\"date_from\",\"date_to\",\"atr_a\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "\n",
        "df.createOrReplaceTempView(\"tbl2\")\n",
        "df.show(10,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSdboP6EjIWL",
        "outputId": "e1c4412c-ad50-4164-e68b-e4c08989d0e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+-----+\n",
            "|id |date_from |date_to   |atr_a|\n",
            "+---+----------+----------+-----+\n",
            "|1  |2000-01-01|2032-01-02|100  |\n",
            "|2  |2010-01-01|2020-01-02|200  |\n",
            "|3  |2011-01-01|2020-01-02|300  |\n",
            "|4  |2018-01-01|2019-01-02|400  |\n",
            "|5  |2019-05-01|2020-05-02|500  |\n",
            "|6  |2020-01-01|2030-02-02|600  |\n",
            "|7  |2021-02-01|2030-01-02|700  |\n",
            "+---+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация с помощью SQL:"
      ],
      "metadata": {
        "id": "qE2GQDIFnL5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM tbl2\n",
        "WHERE date_to >= '2019-03-01' AND date_from <= '2021-03-01'\n",
        "\"\"\"\n",
        "result_df = spark.sql(query)\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgcq86R2jNJ0",
        "outputId": "23234bdd-4cb8-4308-fce8-5be20bd6d5c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+-----+\n",
            "| id| date_from|   date_to|atr_a|\n",
            "+---+----------+----------+-----+\n",
            "|  1|2000-01-01|2032-01-02|  100|\n",
            "|  2|2010-01-01|2020-01-02|  200|\n",
            "|  3|2011-01-01|2020-01-02|  300|\n",
            "|  5|2019-05-01|2020-05-02|  500|\n",
            "|  6|2020-01-01|2030-02-02|  600|\n",
            "|  7|2021-02-01|2030-01-02|  700|\n",
            "+---+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтрация без SQL:"
      ],
      "metadata": {
        "id": "BYyvpLXVnVoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_start = '2019-03-01'\n",
        "d_end = '2021-03-01'\n",
        "res = df.filter((F.col(\"date_from\") <= d_end) & (F.col(\"date_to\") >= d_start))\n",
        "res.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S21L02Nfjvj5",
        "outputId": "a265ea65-6c8e-4419-f244-c4d0c250af6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+-----+\n",
            "| id| date_from|   date_to|atr_a|\n",
            "+---+----------+----------+-----+\n",
            "|  1|2000-01-01|2032-01-02|  100|\n",
            "|  2|2010-01-01|2020-01-02|  200|\n",
            "|  3|2011-01-01|2020-01-02|  300|\n",
            "|  5|2019-05-01|2020-05-02|  500|\n",
            "|  6|2020-01-01|2030-02-02|  600|\n",
            "|  7|2021-02-01|2030-01-02|  700|\n",
            "+---+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача №3"
      ],
      "metadata": {
        "id": "t2qTWmMroELY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Есть таблица абонентов с историей изменений. Каждая запись по абоненту действует в пределах периода c date_from по date_to.\n",
        "Если одна запись закрывается, следующая запись начинается с той же даты.  \n",
        "По каждому абоненту должна быть только одна действующая запись в интервале [date_from, date_to).\n",
        "Необходимо написать запрос, который вернет те записи, у которых некорректная дата закрытия."
      ],
      "metadata": {
        "id": "7UYeI3eMoK_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Набор исходных данных для задачи №3\n",
        "data = ([(\"1\",\"2000-01-01\",\"2000-10-02\"),\n",
        "         (\"1\",\"2000-10-02\",\"2020-01-02\"),\n",
        "         (\"1\",\"2010-01-01\",\"2019-01-02\"),\n",
        "         (\"1\",\"2019-01-02\",\"2023-01-01\"),\n",
        "         (\"2\",\"2019-05-01\",\"2020-05-02\"),\n",
        "         (\"2\",\"2019-05-01\",\"2020-02-10\"),\n",
        "         (\"2\",\"2020-02-10\",\"2020-02-15\"),\n",
        "         (\"2\",\"2020-02-16\",\"2030-01-02\"),\n",
        "         (\"3\",\"2021-02-01\",\"2022-01-01\"),\n",
        "         (\"3\",\"2022-01-01\",\"2023-01-01\"),\n",
        "         (\"3\",\"2023-01-01\",\"2030-01-02\"),\n",
        "         (\"3\",\"2023-01-01\",\"2024-01-02\")\n",
        "       ])\n",
        "columns = [\"id\",\"date_from\",\"date_to\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "\n",
        "df.createOrReplaceTempView(\"tbl3\")\n",
        "df.show(100,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3xTvwKboN1D",
        "outputId": "d627c7e9-2585-45f5-e911-7e382beb461c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "|id |date_from |date_to   |\n",
            "+---+----------+----------+\n",
            "|1  |2000-01-01|2000-10-02|\n",
            "|1  |2000-10-02|2020-01-02|\n",
            "|1  |2010-01-01|2019-01-02|\n",
            "|1  |2019-01-02|2023-01-01|\n",
            "|2  |2019-05-01|2020-05-02|\n",
            "|2  |2019-05-01|2020-02-10|\n",
            "|2  |2020-02-10|2020-02-15|\n",
            "|2  |2020-02-16|2030-01-02|\n",
            "|3  |2021-02-01|2022-01-01|\n",
            "|3  |2022-01-01|2023-01-01|\n",
            "|3  |2023-01-01|2030-01-02|\n",
            "|3  |2023-01-01|2024-01-02|\n",
            "+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tab1 = spark.sql( \"\"\"\n",
        "# SELECT id, date_from, date_to,\n",
        "#     date_to = lag(date_from, -1) over (partition by id ORDER BY id) as correct\n",
        "# from tbl3\n",
        "# \"\"\")\n",
        "# tab1.show()\n",
        "\n",
        "tab2 = spark.sql( \"\"\"\n",
        "SELECT id, date_from, date_to\n",
        "FROM (SELECT id, date_from, date_to,\n",
        "      date_to = (lag(date_from, -1) over (partition by id ORDER BY id)) as correct\n",
        "      from tbl3) t\n",
        "WHERE correct = False\n",
        "\"\"\")\n",
        "\n",
        "tab2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD6ogR-YoYFI",
        "outputId": "40f1de44-08eb-4206-fe75-2583da777f20"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "| id| date_from|   date_to|\n",
            "+---+----------+----------+\n",
            "|  1|2000-10-02|2020-01-02|\n",
            "|  2|2019-05-01|2020-05-02|\n",
            "|  2|2020-02-10|2020-02-15|\n",
            "|  3|2023-01-01|2030-01-02|\n",
            "+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab1 = spark.sql( \"\"\"\n",
        "SELECT id, date_from, date_to,\n",
        "    LAG(date_to) OVER (PARTITION BY id ORDER BY date_from) AS previous_date_to\n",
        "FROM tbl3\n",
        "\"\"\")\n",
        "incorrect_records = tab1.filter(tab1.date_from != tab1.previous_date_to)\n",
        ".show(100, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng9rOvt7ttAc",
        "outputId": "c349fd80-fa0c-43b6-8ffb-809d8af27f04"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+----------------+\n",
            "|id |date_from |date_to   |previous_date_to|\n",
            "+---+----------+----------+----------------+\n",
            "|1  |2010-01-01|2019-01-02|2020-01-02      |\n",
            "|2  |2019-05-01|2020-02-10|2020-05-02      |\n",
            "|2  |2020-02-16|2030-01-02|2020-02-15      |\n",
            "|3  |2023-01-01|2024-01-02|2030-01-02      |\n",
            "+---+----------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задача №4\n",
        "\n",
        "Есть две таблицы. В одной хранится история изменений атрибутов абонента. В другой история изменения атрибутов слицевых счета.\n",
        "Необходимо пересечь данные из этих таблиц таким обазом, чтобы сохранилась история изменения каждого из атрибутов."
      ],
      "metadata": {
        "id": "xZhfb1aByNBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ([(\"1\",\"10\",\"2001-01-01\",\"2010-01-01\"),\n",
        "         (\"1\",\"20\",\"2010-01-01\",\"2017-01-02\"),\n",
        "         (\"1\",\"30\",\"2017-01-01\",\"2020-01-02\")\n",
        "       ])\n",
        "columns = [\"appn\",\"acc\",\"date_from\",\"date_to\"]\n",
        "df_appn = spark.createDataFrame(data = data, schema = columns)\n",
        "df_appn.createOrReplaceTempView(\"tbl_appn\")\n",
        "\n",
        "data = ([(\"10\",\"a\",\"x\",\"2000-01-01\",\"2008-01-01\"),\n",
        "         (\"10\",\"b\",\"x\",\"2008-01-01\",\"2009-01-01\"),\n",
        "         (\"10\",\"b\",\"y\",\"2009-01-01\",\"2012-01-01\"),\n",
        "         (\"20\",\"c\",\"z\",\"2007-01-01\",\"2011-01-02\"),\n",
        "         (\"20\",\"c\",\"w\",\"2011-01-02\",\"2020-01-02\"),\n",
        "         (\"30\",\"d\",\"w\",\"2015-01-02\",\"2018-01-02\"),\n",
        "         (\"30\",\"f\",\"w\",\"2018-01-02\",\"2019-01-02\"),\n",
        "         (\"30\",\"f\",\"h\",\"2019-01-02\",\"2021-01-02\")\n",
        "       ])\n",
        "columns = [\"acc\",\"acc_num\",\"tp\", \"date_from\",\"date_to\"]\n",
        "df_acc = spark.createDataFrame(data = data, schema = columns)\n",
        "df_acc.createOrReplaceTempView(\"tbl_acc\")\n",
        "\n",
        "df_appn.show(200,0)\n",
        "df_acc.show(200,0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-_1XKXvyQYS",
        "outputId": "b7c30b7e-344b-4bd2-b498-b9e1ca79b566"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----------+----------+\n",
            "|appn|acc|date_from |date_to   |\n",
            "+----+---+----------+----------+\n",
            "|1   |10 |2001-01-01|2010-01-01|\n",
            "|1   |20 |2010-01-01|2017-01-02|\n",
            "|1   |30 |2017-01-01|2020-01-02|\n",
            "+----+---+----------+----------+\n",
            "\n",
            "+---+-------+---+----------+----------+\n",
            "|acc|acc_num|tp |date_from |date_to   |\n",
            "+---+-------+---+----------+----------+\n",
            "|10 |a      |x  |2000-01-01|2008-01-01|\n",
            "|10 |b      |x  |2008-01-01|2009-01-01|\n",
            "|10 |b      |y  |2009-01-01|2012-01-01|\n",
            "|20 |c      |z  |2007-01-01|2011-01-02|\n",
            "|20 |c      |w  |2011-01-02|2020-01-02|\n",
            "|30 |d      |w  |2015-01-02|2018-01-02|\n",
            "|30 |f      |w  |2018-01-02|2019-01-02|\n",
            "|30 |f      |h  |2019-01-02|2021-01-02|\n",
            "+---+-------+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = spark.sql(\"\"\"\n",
        "select a.appn, a.acc, b.acc_num, b.tp,\n",
        "       greatest(a.date_from, b.date_from) as date_from,\n",
        "       least(a.date_to, b.date_to) as date_to\n",
        "from tbl_appn a join tbl_acc b on a.acc = b.acc\n",
        "and greatest(a.date_from, b.date_from) < least(a.date_to, b.date_to)\n",
        "order by appn, acc, date_from\n",
        "\"\"\")\n",
        "\n",
        "history.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDPcPMhW2gMF",
        "outputId": "24649cdf-3c67-42be-a1b0-676c038cc954"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-------+---+----------+----------+\n",
            "|appn|acc|acc_num|tp |date_from |date_to   |\n",
            "+----+---+-------+---+----------+----------+\n",
            "|1   |10 |a      |x  |2001-01-01|2008-01-01|\n",
            "|1   |10 |b      |x  |2008-01-01|2009-01-01|\n",
            "|1   |10 |b      |y  |2009-01-01|2010-01-01|\n",
            "|1   |20 |c      |z  |2010-01-01|2011-01-02|\n",
            "|1   |20 |c      |w  |2011-01-02|2017-01-02|\n",
            "|1   |30 |d      |w  |2017-01-01|2018-01-02|\n",
            "|1   |30 |f      |w  |2018-01-02|2019-01-02|\n",
            "|1   |30 |f      |h  |2019-01-02|2020-01-02|\n",
            "+----+---+-------+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}